shift: 96
pertentage_train: 0.7
scaler_inputs: standard
scaler_targets: standard

size_output: 1
size_input: 96
size_hidden: 72
num_layers_lstm: 1
num_layers_linear: [128, 64]
activation_function: relu

num_epochs: 100
learning_rate: 0.001
optimizer: adam
criterion: mse